{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In this exercise, you'll utilize re.search() and re.match() to find specific tokens.\",\n",
       " 'Both search and match expect regex patterns, similar to those you defined in an earlier exercise.',\n",
       " \"You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\",\n",
       " 'You have both scene_one and sentences available from the last exercise; now you can use them with [re.search()] and re.match() to extract and match more text.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import re\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "scene_one = \"In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora. You have both scene_one and sentences available from the last exercise; now you can use them with [re.search()] and re.match() to extract and match more text.\"\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'have',\n",
       " 'both',\n",
       " 'scene_one',\n",
       " 'and',\n",
       " 'sentences',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'last',\n",
       " 'exercise',\n",
       " ';',\n",
       " 'now',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'them',\n",
       " 'with',\n",
       " '[',\n",
       " 're.search',\n",
       " '(',\n",
       " ')',\n",
       " ']',\n",
       " 'and',\n",
       " 're.match',\n",
       " '(',\n",
       " ')',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'and',\n",
       " 'match',\n",
       " 'more',\n",
       " 'text',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 76 <re.Match object; span=(68, 76), match='specific'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"specific\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end(), match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.search()\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one).group()[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_digits_and_words = \"(\\d+|\\w+|\\.)\"\n",
    "\n",
    "re.findall(match_digits_and_words, \"He has 11 cats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "pattern = r\"(\\w+|#\\d|\\?|!)\"\n",
    "regexp_tokenize(my_string, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    "          '#NLP is super fun! <3 #learning',\n",
    "          'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 5),\n",
       " ('and', 5),\n",
       " ('(', 4),\n",
       " (')', 4),\n",
       " ('to', 4),\n",
       " ('.', 4),\n",
       " ('exercise', 3),\n",
       " ('the', 3),\n",
       " ('in', 2),\n",
       " (',', 2)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(word_tokenize(scene_one.lower())).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 5),\n",
       " ('and', 5),\n",
       " ('to', 4),\n",
       " ('exercise', 3),\n",
       " ('the', 3),\n",
       " ('in', 2),\n",
       " ('both', 2),\n",
       " ('match', 2),\n",
       " ('regex', 2),\n",
       " ('text', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w for w in word_tokenize(scene_one.lower()) if w.isalpha()]\n",
    "Counter(tokens).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exercise', 3),\n",
       " ('match', 2),\n",
       " ('regex', 2),\n",
       " ('text', 2),\n",
       " ('utilize', 1),\n",
       " ('find', 1),\n",
       " ('specific', 1),\n",
       " ('tokens', 1),\n",
       " ('search', 1),\n",
       " ('expect', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "no_stops = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "\n",
    "Counter(no_stops).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exercise', 3),\n",
       " ('match', 2),\n",
       " ('regex', 2),\n",
       " ('text', 2),\n",
       " ('utilize', 1),\n",
       " ('find', 1),\n",
       " ('specific', 1),\n",
       " ('token', 1),\n",
       " ('search', 1),\n",
       " ('expect', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words(\"english\")]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "bow.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "my_documents = [\n",
    "    \"The movie was about a spaceship and aliens.\",\n",
    "    \"I really liked the movie!\",\n",
    "    \"Awesome action scenes, but boring characters.\",\n",
    "    \"The movie was awful! I hate alien films.\",\n",
    "    \"Space is cool! I liked the movie.\",\n",
    "    \"More space films, please!\",\n",
    "]\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a bag-of-word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf with gensim\n",
    "\n",
    "Term frequency - inverse document frequency. Allows you to determine the most important words in each document. Each corpus may have shared words beyond just stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.1746298276735174),\n",
       " (7, 0.1746298276735174),\n",
       " (9, 0.1746298276735174),\n",
       " (10, 0.29853166221463673),\n",
       " (11, 0.47316148988815415),\n",
       " (12, 0.7716931521027908)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sort the weights from highest to lowest: sorted_tfidf_weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tfidf_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m sorted_tfidf_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(tfidf_weights, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m w: w[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print the top 5 weighted words\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Organised/python/python/DeepNeuralNetwork/.venv/lib/python3.10/site-packages/gensim/models/tfidfmodel.py:495\u001b[0m, in \u001b[0;36mTfidfModel.__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# unknown (new) terms will be given zero weight (NOT infinity/huge weight,\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# as strict application of the IDF formula would dictate)\u001b[39;00m\n\u001b[1;32m    494\u001b[0m termid_array, tf_array \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m termid, tf \u001b[38;5;129;01min\u001b[39;00m bow:\n\u001b[1;32m    496\u001b[0m     termid_array\u001b[38;5;241m.\u001b[39mappend(termid)\n\u001b[1;32m    497\u001b[0m     tf_array\u001b[38;5;241m.\u001b[39mappend(tf)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "tfidf_weights = tfidf[1]\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
